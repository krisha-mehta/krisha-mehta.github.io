---
layout:     page
title:
permalink:  /
---

<div class="row">
    <div class="col-sm-6 col-xs-12">
        <img src="/img/cover2.jpg">
    </div>
    <div class="col-sm-6 col-xs-12" style="margin-bottom: 0;">
        PhD Student<br>
        University of Chicago<br>
        krisha at uchicago dot edu
        <br><br>
    </div>
</div>
<hr>

<a name="/bio"></a>

# Bio

Welcome!

I am Krisha Mehta, a first year PhD student at the Computer Science department at the University of Chicago. I am advised by Prof. Alex Kale at the Data Cognition Lab. I am interested in studying how visualizations can help people reason about uncertainty and drive their decision-making. 

Apart from that, I love to buy and read books (yes, those are two separate hobbies) and can cook a mean daal makhani. Occasionally, I summarize research papers on a blog called Computers, Papers and Everything. 

My CV is available [here][33].


<a name="/news"></a>

# News
- [Sept 2023] Awarded the Liew Family Graduate Fellowship.
- [Sept 2023] Joined the PhD in CS program at the University of Chicago!
- [Oct 2022] Got promoted to Programmer writer II.
- [June 2022] Worked on Ground Truth Plus and Ground Truth Synthetic.
- [June 2021] Started working full-time at Amazon SageMaker as a programmer writer. 
- [May 2021] Graduated from Arizona State University with a Masters in Computer Science
- [Aug 2020] Interned at Weights & Biases as a machine learning engineer.
- [May 2020] Interned as a technical writer intern at Amazon this summer.
- [Feb 2020] Our paper [IndoorNet: Generating Indoor Layouts from a Single Panorama Image][IndoorNet] got published in Advanced Computing Technologies and Applications. Algorithms for Intelligent Systems. Springer, Singapore, 2020
- [Aug 2019] Joined the MS in CS program at Arizona State University.
- [June 2019] Selected to join the technical student program at The European Organization for Nuclear Research(CERN)(declined)
- [May 2019] Our paper [HomeNet: Layout Generation of Indoor Scenes from Panoramic Images Using Pyramid Pooling][HomeNet] got accepted at the 3D Scene Generation Workshop, CVPR, 2019!
- [Nov 2018] Selected for the Summer Research Program by the University of Auckland, New Zealand.

<!--<div id="read-more-button">
    <a nohref>Read more</a>
</div> -->

<hr>


<!--<div class="row" id="timeline-logos">
<div class="col-xs-2">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="https://www.asu.edu/"><img src="/img/logos/asu.png"></a>
        </div>
        <div class="logo-desc">
            Arizona State University<br>
            2019 -  2021
        </div>
    </div>
    <div class="col-xs-2">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="http://www.djsce.ac.in/"><img src="/img/logos/djsce.png"></a>
        </div>
        <div class="logo-desc">
            D.J Sanghvi College of Engineering<br>
            2015 - 2019
        </div>
    </div>
    <div class="col-xs-2">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="https://www.auckland.ac.nz/en.html"><img style="width:160px,height:160px;" src="/img/logos/uoa.png"></a>
        </div>
        <div class="logo-desc">
            The University of Auckland<br>
            Winter 2018
        </div>
    </div>
    <div class="col-xs-2">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="https://www.iima.ac.in/web/iima"><img style="width:120px,height:120px;" src="/img/logos/iim.png"></a>
        </div>
        <div class="logo-desc">
            Indian Institute of Management, Ahmedabad<br>
            Winter 2017
        </div>
    </div> 
    <div class="col-xs-2">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="//vt.edu"><img src="/img/logos/vt.png"></a>
        </div>
        <div class="logo-desc">
            Virginia Tech<br>
            2015 - 2016
        </div>
    </div> 
   <div class="col-xs-2">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="https://www.iiit.ac.in/"><img style="height: 160px,width:160px;" src="/img/logos/iiit.png"></a>
        </div>
        <div class="logo-desc">
            Summer School at IIIT-H<br>
            Summer 2018
        </div>
    </div> 
    
  <div class="col-xs-2">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//deepmind.com"><img style="width:120px;" src="/img/logos/deepmind.png"></a>
        </div>
        <div class="logo-desc">
            DeepMind<br>
            W2019
        </div>
    </div> 
</div>-->

<a name="/publications"></a>

# Publications

<a name="/multi-agent-comm"></a>
<h2 class="pubt">HomeNet: Layout Generation of Indoor Scenes from Panoramic Images Using Pyramid Pooling</h2>
<p class="pubd">
    <span class="authors">Krisha Mehta, Yash Kotadia</span><br>
    <span class="conf">3D Scene Generation Workshop, CVPR, 2019</span><br>
    <span class="links">
        <a target="_blank" href="https://drive.google.com/file/d/1KYcuWqcSxQ_PeK5FOnjS40cKc9wALjak/view">Paper</a>
    </span>
</p>
<img src="/img/HomeNet/Flowchart.png">
<!--<br><br> -->
<!--<img src="/img/multi-agent-comm/shapes.gif"> -->

<!--<hr>

<a name="/eqa-mp3d"></a>
<h2 class="pubt">IndoorNet: Generating Indoor Layouts from a Single Panorama Image</h2>
<p class="pubd">
    <span class="authors">
        Yash Kotadia, Krisha Mehta, Mihir Manjrekar, Ruhina Karani
    </span><br>
    <span class="conf"></span><br>
    <span class="links"> 
       <a target="_blank" href="https://arxiv.org/abs/1904.03461">Paper</a> 
    </span>
</p>
<img src="/img/IndoorNet/Image.png"> -->

<!--<a name="/avsd"></a>
<h2 class="pubt">Audio-Visual Scene-Aware Dialog</h2>
<p class="pubd">
    <span class="authors">
        Huda Alamri, Vincent Cartillier, Abhishek Das,
        Jue Wang, Stefan Lee, Peter Anderson, Irfan Essa, Devi Parikh,
        Dhruv Batra, Anoop Cherian, Tim K. Marks, Chiori Hori
    </span><br>
    <span class="conf">CVPR 2019</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1901.09107">Paper</a>
        <a target="_blank" href="http://video-dialog.com/">video-dialog.com</a>
    </span>
</p>
<img src="/img/avsd/avsd.jpg">
<hr>

<a name="/avsd_icassp"></a>
<h2 class="pubt">End-to-end Audio Visual Scene-Aware Dialog Using Multimodal Attention-based Video Features</h2>
<p class="pubd">
    <span class="authors">
            Chiori Hori, Huda Alamri, Jue Wang, Gordon Wichern, Takaaki Hori, Anoop Cherian, Tim K. Marks, Vincent Cartillier, Raphael Lopes, Abhishek Das, Irfan Essa, Dhruv Batra, Devi Parikh
    </span><br>
    <span class="conf">ICASSP 2019</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1806.08409">Paper</a>
        <a target="_blank" href="http://video-dialog.com/">video-dialog.com</a>
    </span>
</p>
<img src="/img/avsd/avsd_icassp.jpg"> 
<hr> -->

<!--<a name="/eqa-modular"></a>
<h2 class="pubt">Neural Modular Control for Embodied Question Answering</h2>
<p class="pubd">
    <span class="authors">Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra</span><br>
    <span class="conf">CoRL 2018 (Spotlight)</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1810.11181">Paper</a>
        <a target="_blank" href="https://embodiedqa.org/">embodiedqa.org</a>
        <a target="_blank" href="https://www.youtube.com/watch?v=xoHvho-YRgs&t=7330">Presentation video</a>
    </span>
</p>

<img src="/img/eqa/eqa-modular.png">

<hr> -->
<!--<a name="/embodied-qa"></a>
<h2 class="pubt">Embodied Question Answering</h2>
<p class="pubd">
    <span class="authors">Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra</span><br>
    <span class="conf">CVPR 2018 (Oral)</span><br>
    <span class="links">
        <a target="_blank" href="https://embodiedqa.org/paper.pdf">Paper</a>
        <a target="_blank" href="https://embodiedqa.org/">embodiedqa.org</a>
        <a target="_blank" href="https://github.com/facebookresearch/EmbodiedQA">Code</a>
        <a target="_blank" href="//youtu.be/gz2VoDrvX-A?t=1h29m14s">Presentation video</a>
    </span>
    <div class="row pressdiv" style="margin: 5px 0 0 0; line-height: 1.4em;">
        <a style="border-bottom: 0;" target="_blank" href="https://mlatgt.blog/2018/02/26/embodied-question-answering/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/mlgt.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Embodied Question Answering" by Abhishek Das</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://code.facebook.com/posts/1622140391226436/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/fair2.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"... a goal-driven approach to autonomous agents" by Dhruv Batra, Devi Parikh</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://www.technologyreview.com/s/611040/facebook-helped-create-an-ai-scavenger-hunt-that-could-lead-to-the-first-useful-home-robots/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 3px 0;">
                <img src="/img/logos/mittechreview.svg" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"... an AI scavenger hunt that could lead to the first useful home robots" by Will Knight</span>
            </div>
        </a>
    </div>
</p>

<img src="/img/eqa/teaser.jpg"> 

<hr> -->
<!--<h2 class="pubt">Evaluating Visual Conversational Agents via Cooperative Human-AI Games</h2>
<p class="pubd">
    <span class="authors">Prithvijit Chattopadhyay<sup>*</sup>, Deshraj Yadav<sup>*</sup>, Viraj Prabhu, Arjun Chandrasekaran, Abhishek Das, Stefan Lee, Dhruv Batra, Devi Parikh</span><br>
    <span class="conf">HCOMP 2017</span><br>
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1708.05122">Paper</a>
        <a target="_blank" href="//github.com/VT-vision-lab/guesswhich">Code</a>
    </span>
</p>

<img src="/img/guesswhich/teaser.jpg">

<a name="/visdial-rl"></a> -->

<!--<hr>
<h2 class="pubt">Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning</h2>
<p class="pubd">
    <span class="authors">Abhishek Das<sup>*</sup>, Satwik Kottur<sup>*</sup>, Stefan Lee, José M.F. Moura, Dhruv Batra</span><br>
    <span class="conf">ICCV 2017 (Oral)</span><br>
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1703.06585">Paper</a>
        
        <a target="_blank" href="//www.youtube.com/watch?v=R4hugGnNr7s">Presentation video</a>
    </span>
</p>

<img src="/img/visdial/qbot_abot.jpg">

<hr> -->
<!--<h2 class="pubt">Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</h2>
<p class="pubd">
    <span class="authors">Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra</span><br>
    <span class="conf">ICCV 2017, NIPS 2016 Interpretable ML for Complex Systems Workshop</span><br>
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1610.02391">Paper</a>
        <a target="_blank" href="https://github.com/ramprs/grad-cam">Code</a>
        <a target="_blank" href="http://gradcam.cloudcv.org/">Demo</a>
    </span>
</p>

<img src="/img/grad-cam/teaser.png">

<a name="/visdial"></a>

<hr>
<h2 class="pubt">Visual Dialog</h2>
<p class="pubd" style="margin-bottom:20px;">
    <span class="authors">Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José M.F. Moura, Devi Parikh, Dhruv Batra</span><br>
    <span class="conf">PAMI 2018, CVPR 2017 (Spotlight)</span><br>
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1611.08669">Paper</a>
        <a target="_blank" href="//github.com/batra-mlp-lab/visdial">Code</a>
        <a target="_blank" href="http://visualdialog.org/">visualdialog.org</a>
        <a target="_blank" href="https://github.com/batra-mlp-lab/visdial-amt-chat">AMT chat interface</a>
        <a target="_blank" href="http://demo.visualdialog.org">Demo</a>
        <a target="_blank" href="//www.youtube.com/watch?v=I9OlorMh7wU">Presentation video</a>
    </span>
</p>

<img src="/img/visdial/teaser.png"> -->

<!-- <div id="vimeo-embed">
    <iframe src="https://player.vimeo.com/video/193092429?byline=0&portrait=0&color=ffffff" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div> -->

<!--<h2 class="pubt">Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?</h2>

<p class="pubd">
    <span class="authors">Abhishek Das<sup>*</sup>, Harsh Agrawal<sup>*</sup>, C. Lawrence Zitnick, Devi Parikh, Dhruv Batra</span> <br>
    <span class="conf">CVIU 2017, EMNLP 2016, ICML 2016 Workshop on Visualization for Deep Learning</span><br>
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1606.03556">Paper</a>
        <a target="_blank" href="https://abhishekdas.com/vqa-hat/">Project+Dataset</a>
        <a target="_blank" href="https://github.com/abhshkdz/neural-vqa-attention">neural-vqa-attention</a>
    </span>
    <div class="row pressdiv" style="margin: 5px 0 0 0; line-height: 1.4em;">
        <a style="border-bottom: 0;" target="_blank" href="http://nautil.us/issue/40/learning/is-artificial-intelligence-permanently-inscrutable">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/nautilus.png" style="background: white; width: 57px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Is Artificial Intelligence Permanently Inscrutable?" by Aaron Bornstein</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="http://www.theverge.com/2016/7/12/12158238/first-click-deep-learning-algorithmic-black-boxes">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/theverge.png" style="margin-right: 5px; background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Deep learning is creating computer systems we don't fully understand" by James Vincent</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://www.newscientist.com/article/2095616-robot-eyes-and-humans-fix-on-different-things-to-decode-a-scene/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/newscientist.jpg" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Robot eyes and humans fix on different things to decode a scene" by Aviva Rutkin</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="http://www.techradar.com/news/world-of-tech/robots-and-humans-see-the-world-differently-but-we-don-t-know-why-1324165">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/techradar.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Robots and humans see the world differently – but we don't know why" by Duncan Geere</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://www.technologyreview.com/s/601819/ai-is-learning-to-see-the-world-but-not-the-way-humans-do/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 3px 0;">
                <img src="/img/logos/mittechreview.svg" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"AI Is Learning to See the World—But Not the Way Humans Do" by Jamie Condliffe</span>
            </div>
        </a>
    </div>
</p><img src="/img/vqa-hat/teaser.jpg">
<hr> -->

<!--<a name="/talks"></a>

# Talks

<div class="row">
    <div class="col-xs-6">
        <p class="talkd">
            <img src="/img/talks/visdial_rl_iccv17.jpg">
        </p>
    </div>
    <div class="col-xs-6">
        <p class="talkd">
            <img src="/img/talks/embodiedqa_cvpr18_4.jpg">
        </p>
    </div>
</div>
<div class="row">
    <div class="col-xs-12">
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=WxYBp3Xr_Nc">
                Allen Institute for Artificial Intelligence: "Towards Agents that can See, Talk, and Act"
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=xoHvho-YRgs&t=7330">
                CoRL 2018 Spotlight: Neural Modular Control for Embodied Question Answering
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://youtu.be/gz2VoDrvX-A?t=1h29m14s">
                CVPR 2018 Oral: Embodied Question Answering
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="http://on-demand.gputechconf.com/gtc/2018/video/S8582/">
                NVIDIA GTC 2018
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=R4hugGnNr7s">
                ICCV 2017 Oral: Learning Cooperative Visual Dialog Agents with Deep RL
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://youtu.be/KAlGWMJnWyc?t=26m56s">
                Visual Question Answering Challenge Workshop, CVPR 2017
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=I9OlorMh7wU">
                CVPR 2017 Spotlight: Visual Dialog
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="http://techtalks.tv/talks/towards-transparent-visual-question-answering-systems/63026/">
                Visualization for Deep Learning Workshop, ICML 2016
            </a>
        </div>
        
    </div>
</div>
<hr> -->
<hr>

<!--
<a name="/projects"></a>

# Side projects

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="http://aideadlin.es">aideadlin.es</a></h2>
        <p class="talkd">
            aideadlin.es is a webpage to keep track of CV/NLP/ML/AI conference deadlines. It's hosted on GitHub, and countdowns are automatically updated via pull requests to the data file in the repo.
            <a target="_blank" href="http://aideadlin.es"><img style="margin-top: 10px;" src="/img/projects/ai-deadlines-1547012831.png"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/neural-vqa-attention">neural-vqa-attention</a></h2>
        <p class="talkd">
            Torch implementation of an attention-based visual question answering model (Yang et al., CVPR16).
            The model looks at an image, reads a question, and comes up with an answer to the question and a heatmap of where it looked in the image to answer it.
            Some results <a href="https://computing.ece.vt.edu/~abhshkdz/neural-vqa-attention/figures/">here</a>.
            <a target="_blank" href="https://github.com/abhshkdz/neural-vqa-attention"><img class="project-img" src="/img/projects/neural-vqa-attention.jpg"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/neural-vqa">neural-vqa</a></h2>
        <p class="talkd">
            neural-vqa is an efficient, GPU-based Torch implementation of the visual question answering model from the NIPS 2015 paper 'Exploring Models and Data for Image Question Answering' by Ren et al.
            <a target="_blank" href="https://github.com/abhshkdz/neural-vqa"><img src="/img/projects/neural-vqa.jpg"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://erdos.sdslabs.co">Erdős</a></h2>
        <p class="talkd">
            Erdős by <a target="_blank" href="//sdslabs.co">SDSLabs</a> is a competitive math learning platform, similar in spirit to <a href="https://projecteuler.net/">Project Euler</a>, albeit more feature-packed (support for holding competitions, has a social layer) and prettier.
            <a target="_blank" href="https://erdos.sdslabs.co"><img style="margin-top:10px;" src="/img/projects/erdos.jpg"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/graf">graf</a></h2>
        <p class="talkd">
            graf plots pretty git contribution bar graphs in the terminal.
            <code>gem install graf</code> to install.
            <a target="_blank" href="https://github.com/abhshkdz/graf"><img style="margin-top:10px;" src="/img/projects/graf.gif"></a>
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/HackFlowy">HackFlowy</a></h2>
        <p class="talkd">
            Clone of <a href="//workflowy.com">WorkFlowy.com</a>, a beautiful, list-based note-taking website that has a 500-item monthly limit on the free tier :-(. This project is an open-source clone of WorkFlowy. "Make lists. Not war." :-)
            <a target="_blank" href="https://github.com/abhshkdz/HackFlowy"><img style="margin-top:40px;" src="/img/projects/hackflowy.png"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/AirMaps">AirMaps</a></h2>
        <p class="talkd">
            AirMaps was a fun hackathon project that lets users navigate through Google Earth with gestures and speech commands using a Kinect sensor. It was the <a target="_blank" href="https://blog.sdslabs.co/2014/02/code-fun-do">winning entry in Microsoft Code.Fun.Do</a>.
            <a target="_blank" href="https://github.com/abhshkdz/AirMaps"><img style="margin-top:10px;" src="/img/projects/airmaps.jpg"></a>
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/sdslabs/hackview">HackView</a></h2>
        <p class="talkd">
            Another fun hackathon-winning project built during Yahoo! HackU! 2012 that involves webRTC-based P2P video chat, and was faster than any other video chat provider (at the time, before Google launched Hangouts).
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/8tracks-downloader">8tracks-downloader</a></h2>
        <p class="talkd">
            Ugly-looking, but super-effective bash script for downloading entire playlists from 8tracks. (Still works as of 10/2016).
        </p>
    </div>
</div> -->

<script src="/js/jquery.min.js"></script>
<script type="text/javascript">
    $('ul:gt(0) li:gt(12)').hide();
    $('#read-more-button > a').click(function() {
        $('ul:gt(0) li:gt(12)').show();
        $('#read-more-button').hide();
    });
</script>

---

[1]: //mlp.cc.gatech.edu
[2]: ///www.cc.gatech.edu/~dbatra/
[3]: //www.cc.gatech.edu/~parikh/
[4]: //www.qbi.uq.edu.au/professor-geoffrey-goodhill
[5]: //researchers.uq.edu.au/researcher/2490
[6]: http://cns.qbi.uq.edu.au/
[7]: //developers.google.com/open-source/gsoc/
[8]: /posts/summer-of-code/
[9]: /posts/gsoc-reunion-2014/
[10]: //blog.sdslabs.co/2012/09/hacku
[11]: //blog.sdslabs.co/2014/02/code-fun-do
[12]: //www.facebook.com/SDSLabs/posts/527540147292475
[13]: /posts/deloitte-cctc-3/
[14]: /posts/google-india-community-summit/
[15]: //blog.sdslabs.co/2013/10/syntax-error-2013
[16]: //sdslabs.co/
[17]: //erdos.sdslabs.co/
[18]: //projecteuler.net/
[19]: //github.com/abhshkdz/neural-vqa
[20]: //github.com/abhshkdz/HackFlowy
[21]: //github.com/abhshkdz/graf
[22]: //github.com/abhshkdz
[23]: //twitter.com/abhshkdz
[24]: //instagram.com/abhshkdz
[25]: http://x.abhishekdas.com/
[26]: https://abhishekdas.com/vqa-hat/
[27]: http://arxiv.org/abs/1606.03556
[28]: https://www.newscientist.com/article/2095616-robot-eyes-and-humans-fix-on-different-things-to-decode-a-scene/
[29]: https://www.technologyreview.com/s/601819/ai-is-learning-to-see-the-world-but-not-the-way-humans-do/
[30]: http://www.theverge.com/2016/7/12/12158238/first-click-deep-learning-algorithmic-black-boxes
[31]: http://iitr.ac.in/
[32]: https://www.facebook.com/dhruv.batra.1253/posts/1783087161932290
[33]: https://drive.google.com/file/d/1n2lqmD80GiSHZOyORRe6314eX_Ax3uT3/view?usp=sharing
[34]: http://aideadlin.es/
[35]: //github.com/abhshkdz/neural-vqa-attention
[36]: https://snapresearchfellowship.splashthat.com/
[37]: https://www.youtube.com/watch?v=R4hugGnNr7s
[38]: https://www.youtube.com/watch?v=I9OlorMh7wU
[39]: https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/
[40]: https://embodiedqa.org/
[41]: https://youtu.be/KAlGWMJnWyc?t=26m56s
[42]: https://2018gputechconf.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=152715
[43]: https://www.ic.gatech.edu/news/600684/three-ic-students-earn-snap-research-awards
[44]: https://www.ic.gatech.edu/news/601084/new-research-fellowships-offer-two-students-funding-access-adobes-creative-cloud
[45]: https://github.com/facebookresearch/House3D
[46]: https://gkioxari.github.io/
[47]: https://research.fb.com/people/parikh-devi/
[48]: https://research.fb.com/people/batra-dhruv/
[49]: https://lvatutorial.github.io/
[50]: http://acl2018.org/tutorials/#connecting-language-and-vis
[51]: http://visualqa.org/workshop.html
[52]: http://on-demand.gputechconf.com/gtc/2018/video/S8582/
[53]: https://visualdialog.org/challenge/2018
[54]: https://youtu.be/gz2VoDrvX-A?t=1h29m14s
[55]: https://research.fb.com/people/rabbat-mike/
[56]: https://www.cs.mcgill.ca/~jpineau/
[57]: https://visualdialog.org/challenge/2018#winners
[58]: https://www.youtube.com/watch?v=xoHvho-YRgs&t=7330
[fb-fellow-page]: https://research.fb.com/announcing-the-2019-facebook-fellows-and-emerging-scholars/
[joelle-corl18-talk-mention]: https://www.youtube.com/watch?v=FSsEqEJKo8A&t=3497
[visdial-challenge-2]: https://visualdialog.org/challenge/2019
[ic-gt-article]: https://www.ic.gatech.edu/news/617061/see-and-say-abhishek-das-working-provide-crucial-communication-tools-intelligent-agents
[HomeNet]: https://drive.google.com/file/d/1KYcuWqcSxQ_PeK5FOnjS40cKc9wALjak/view
[IndoorNet]: https://link.springer.com/chapter/10.1007/978-981-15-3242-9_6










